% === INTRO === %

\vspace{1em}
\subsection{Métodos iterativos}
Los \textit{métodos iterativos} son procedimientos que nos permiten resolver algunos sistemas de ecuaciones lineales del tipo $\mathbf{A}x = b$. Contrario a los \textit{métodos exactos} ---como la \textit{Eliminación Gaussiana}--- que obtienen su resultado en un número finito de pasos, los métodos iterativos generan una sucesión $\{ x^{(k)} \}_{k \in \mathbb{N}_0}$ que, de converger, lo hace a la solución del sistema.

\vspace{1em}
Como esquema básico, dado un $x^{(0)}$ inicial, se define de manera genérica una sucesión iterativa  $\{ x^{(k)} \}_{k \in \mathbb{N}_0}$ de la siguiente manera:

\begin{equation}\label{sucesion}
    x^{(k+1)} = \mathbf{T}x^{(k)} + c
\end{equation}

\vspace{1em}
\noindent donde $\mathbf{T}$ se denomina \textit{matriz de iteración} y $c$ es un vector. En particular, $x^{(k)}$ va a converger a la solución de un sistema, para cualquier vector $x^{(0)}$ inicial, si y sólo si el radio espectral de la matriz de iteración \textbf{T} es menor a 1. Es decir:

\begin{equation}\label{espectral}
    \rho(\mathbf{T}) = max\{|\lambda|\ :\ \lambda \: \text{\textit{autovalor de} \textbf{T}}\}\ <\ 1
\end{equation}

\vspace{3em}
En este informe, trabajaremos con los métodos de \textit{Jacobi} y \textit{Gauss-Seidel} para la resolución de sistemas $\mathbf{A}x = b$. Estos descomponen a la matriz \textbf{A} de la siguiente forma: 

\begin{equation}
    \mathbf{A} = \mathbf{D} - \mathbf{L} - \mathbf{U}
\end{equation}

\vspace{1em}
\noindent donde $\mathbf{D}$ es la diagonal de $\mathbf{A}$, $\mathbf{L}$ contiene los elementos negados por debajo de la misma y $\mathbf{U}$ los elementos negados por encima. 

\vspace{1em}
\noindent Los esquemas para ambos métodos son los siguientes:

\vspace{1em}
\begin{center}
    \textit{Método de Jacobi}
\end{center}

\begin{equation} \label{jacobi}
    x^{(k+1)} = \textbf{D}^{-1} (\textbf{L} + \textbf{U}) x^{(k)} + \textbf{D}^{-1} b 
\end{equation}

\vspace{2em}
\begin{center}
    \textit{Método de Gauss-Seidel}
\end{center}

\begin{equation}\label{gauss-seidel}
    x^{(k+1)} = (\mathbf{D} - \mathbf{L})^{-1} \mathbf{U} x^{(k)} + (\mathbf{D} - \mathbf{L})^{-1} b
\end{equation}

\vspace{1em}
Se puede demostrar que, de converger, ambos métodos lo harán a la solución del sistema pedido. Requerimos adicionalmente, para su aplicación, que $\mathbf{A}$ sea una matriz sin elementos nulos en la diagonal. De lo contrario, no se podrán calcular las inversas de $\mathbf{D}$ y $\mathbf{D} - \mathbf{L}$.





% === IMPLEMENTACION === %

\vspace{2em}
\subsection{Implementación}

% jacobi
\vspace{2em}
\subsubsection{Método de Jacobi}
Definamos la siguiente aridad para una implementación posible del Método de Jacobi:

\begin{align*}
    \text{\textit{jacobi}}&:\ \text{\textit{matriz}}_{n \times n}\ \mathbf{A}\ \times\ \text{\textit{vector}}_n\ \text{b}\ \times\ \text{\textit{nat} q}\ \times\ \text{\textit{real} t}\
    \longrightarrow\ \text{\textit{vector}}_n\ \text{x}
\end{align*}

\vspace{1em}
\noindent donde $n$ es un natural, \textbf{A} es una matriz con elementos distintos a cero en la diagonal, \textit{q} es un número que indica la cantidad máxima de iteraciones a realizar y \textit{t} $\geq$ 0 representa la tolerancia mínima a partir de la que se considera la convergencia de una solución.

\vspace{1em}
Si la matriz $\mathbf{A} = \mathbf{D} - \mathbf{L} - \mathbf{U}$, satisface que $\rho(\mathbf{D}^{-1}(\mathbf{L} + \mathbf{U})) < 1$, entonces el método de Jacobi convergerá a una solución del sistema $\mathbf{A}x = b$. Proponemos el siguiente algoritmo:

\vspace{1em}
\lstinputlisting[mathescape=true, escapechar=@, language=pseudo, label=algo_jacobi, caption={Pseudocódigo para el \textit{Método de Jacobi}.}]{files/src/.code/jacobi.pseudo}

\vspace{1em}
Notamos que la complejidad del algoritmo es del orden de $\Theta(q * n^2)$ en el peor caso. En consecuencia, se debe precisar con cuidado la cantidad de iteraciones a realizar para que el factor $q$ sea despreciable. Del mismo modo, una selección de $t$ correcta, acorde al uso, puede resultar en mejoras considerables en la complejidad promedio. % Como se precisará más adelante, notamos en nuestra experimentación que para $n \leq 3000$ bastó de manera holgada $q = 100$ para lograr un error absoluto $L_1$ menor a $10^{-5}$.


    

% gauss seidel
\vspace{2em} 
\subsubsection{Método de Gauss-Seidel}
De manera similar, definimos la siguiente función que implementa el Método de Gauss-Seidel:

\begin{align*}
    \text{\textit{gauss\_seidel}}&:\ \text{\textit{matriz}}_{n \times n}\ \mathbf{A}\ \times\ \text{\textit{vector}}_n\ \text{b}\ \times\ \text{\textit{nat} q}\ \times\ \text{\textit{real} t}\
    \longrightarrow\ \text{\textit{vector}}_n\ \text{x}
\end{align*}

\vspace{1em}
\noindent donde, nuevamente, $n$ es un natural, \textbf{A} es una matriz con elementos distintos a cero en la diagonal, \textit{q} es un número que indica la cantidad máxima de iteraciones a realizar y $t \geq 0$  representa la tolerancia mínima a partir de la que se considera la convergencia de una solución.

\vspace{1em}
Si la matriz $\mathbf{A} = \mathbf{D} - \mathbf{L} - \mathbf{U}$, satisface que $\rho((\mathbf{D} - \mathbf{L})^{-1}(\mathbf{U})) < 1$, entonces el método de Gauss-Seidel convergerá a una solución del sistema $\mathbf{A}x = b$. Proponemos el siguiente algoritmo, similar al anterior:

\vspace{1em}
\lstinputlisting[mathescape=true, escapechar=@, language=pseudo, label=algo_jacobi, caption={Pseudocódigo para el \textit{Método de Gauss-Seidel}.}]{files/src/.code/gauss_seidel.pseudo}

\vspace{1em}
\noindent Nuevamente, su complejidad temporal es $\Theta(q * n^2)$ en el peor caso.

\vspace{1em}
Observamos que ambos algoritmos aplican ciertas heurísticas que pueden ayudar a reducir su complejidad temporal. Por un lado, se utiliza un vector inicial aleatorio con norma $L_2 = 1$. Esto nos permite evitar aquellas entradas que causan un comportamiento de peor caso de manera determinística, a cuestas que una ejecución particular del algoritmo pueda resultar menos eficiente de manera aleatoria. Por el otro lado, se considera la norma $L_2$ entre dos soluciones consecutivas, como medida de similitud, para definir un quiebre temprano en la iteración externa de los algoritmos en función del parámetro $t$. 




% gauss elim
\vspace{2em}
\subsubsection{Eliminación Gaussiana y representación de matrices}

Por último, para poder realizar una comparación entre los \textit{métodos iterativos} y los \textit{métodos exactos}, propondremos una representación de matriz adecuada y un algoritmo eficiente de \textit{eliminación gaussiana} con \textit{sustitución inversa} para la resolución de \textit{PageRank}.

\vspace{1em}
Dadas las cualidades ralas de las matrices asociadas al problema, decidimos trabajar con la siguiente representación: un vector de vectores \textit{sparse} de la librería \textit{Eigen}, que se implementa sobre un esquema CRS ---\textit{compressed row storage}--- para el indexado a memoria. En el apartado (\ref{representacion}.) explicamos en más detalle esta decisión.

\vspace{1em}
En tanto su implementación, proponemos los siguientes algoritmos para \textit{eliminación gaussiana} y \textit{sustitución inversa}, que aprovechan las herramientas que brinda \textit{Eigen}:

\vspace{1em}
\lstinputlisting[mathescape=true, language=pseudo, label=e, caption={Pseudocódigo para la \textit{Eliminación Gaussiana}.}]{files/src/.code/eg2.pseudo}

\vspace{1em}
\noindent donde \textit{filas}(\textbf{A}) refiere a una función que retorna la cantidad de filas en la matriz \textbf{A}, \textit{v.coeff(i)} refiere a un acceso al $i$-ésimo elemento del vector $v$ y \textit{v.pruned($\alpha$, $\varepsilon$)} es una función que reemplaza los elementos explícitos en la memoria del vector $v$ con valor absoluto menor a $\alpha \cdot \varepsilon$ con un cero implícito.

\vspace{1em}
Notamos que la utilización de la función \textit{pruned} permite mantener una matriz esparsa a lo largo del proceso de eliminación a cuestas de una pérdida de precisión en los resultados. Por ello, una buena selección de $\varepsilon$ es fundamental para el buen funcionamiento del algoritmo. 

\vspace{1em}
\noindent %Para la \textit{sustitución inversa}, por su parte, proponemos el siguiente código:

\vspace{1em}
\lstinputlisting[mathescape=true, language=pseudo, label=sinv, caption={Pseudocódigo para \textit{Sustitución Inversa}.}]{files/src/.code/sinv.pseudo}

\vspace{1em}
El algoritmo (\ref{sinv}.), por su parte, utiliza las funciones \textit{iterador(v)}, que retorna un iterador sobre los elementos no nulos del vector $v$, y \textit{pos(it)}, que retorna la posición ---contando ceros--- a la que está apuntando el iterador $it$. 




\vspace{2em}
\subsubsection{¿Por qué usamos \textit{vector$<$SparseVector$>$} como representación?}\label{representacion}

\vspace{1em}
Nuestra primera implementación `naive' de la \textit{Eliminación Gaussiana} con Eigen fue la siguiente:

\vspace{1em}
\lstinputlisting[mathescape=true, language=pseudo, label=e, caption={Código C++ de la primera implementación de \textit{Eliminación Gaussiana}.}]{files/src/.code/eg1.pseudo}

\vspace{1em}
A pesar de ser un código sencillo, creíamos que al usar las funciones de la librería y la representación \textit{SparseMatrix} podríamos obtener una ventaja en velocidad. Pero la hipótesis fue claramente refutada con los tiempos que obtuvimos: el test \textit{15\_segundos}, por su cuenta, demoró tres minutos y trece segundos en ejecutar en una de nuestras máquinas.

\vspace{1em}
Observamos que la asignación de la línea 13 es una operación muy ineficiente. Esto tiene bastante sentido, ya que \textit{SparseMatrix} está implementada usando \textit{CRS} y por ende todos los elementos se encuentran contiguos en un único vector. Insertar y editar un índice en el medio de éste es un proceso costoso.
 
\vspace{1em}
Concluimos de la observación anterior que, en caso de tener un \textit{vector$<$SparseVector$>$} ---donde podamos hacer reemplazos de una fila por otra considerablemente rápido---, además de seguir aprovechando las operaciones optimizadas que provee \textit{Eigen}, podríamos acelerar considerablemente el algoritmo. Teniendo esto en mente, probamos la implementación detallada en el apartado anterior.

Esta estrategia resultó considerablemente mejor, ya que ---en nuestras máquinas--- resuelve correctamente el test \textit{15\_segundos} en aproximadamente 2.5 segundos y el test \textit{30\_segundos} en 5\footnote{Un comentario necesario es que los tiempos mencionados fueron calculados usando $\varepsilon = 10^{-5}$. Esto es relevante ya que modificar esta variable cambia considerablemente la velocidad del algoritmo. Por ejemplo, con $\varepsilon = 10^{-4}$ el algoritmo termina ambos tests en menos de un segundo, pero con $\varepsilon = 10^{-6}$, demora 3 segundos en el de 15 y 8 en el de 30. No profundizaremos en cómo varía la velocidad respecto a $\varepsilon$.
}.

% \vspace{1em}
% Otra optimización que consideramos fue implementar la resta realizada en la línea 9 del algoritmo. Usando los iteradores de Eigen y un vector auxiliar como se muestra en la siguiente figura.

% \vspace{1em}
% \lstinputlisting[mathescape=true, language=pseudo, label=e, caption={Pseudocódigo de la segunda implementación de la Eliminación Gaussiana}]{files/src/.code/eg3.pseudo}

% \vspace{1em}
% Esta implementación es un poco más rápida ya que corre el test de 15 en 2 segundos y el de 30 en 4, pero no nos parecio una diferencia tal que justifique la pérdida de claridad en el código, por lo que decidimos dejar la implementación anterior.

%\vspace{1em}
%Cabe destacar que para el \textit{tp1} realizamos una implementación similar, usando un vector de vectores de \textit{pair} como representación. Sin embargo, esta resultó más lenta que la versión final usando \textit{Eigen}, probablemente se deba al manejo de memoria y la velocidad de los iteradores de la librería.
